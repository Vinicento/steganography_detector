{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f910b1c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-15T11:22:17.158576Z",
     "iopub.status.busy": "2024-05-15T11:22:17.158129Z",
     "iopub.status.idle": "2024-05-15T22:32:26.385502Z",
     "shell.execute_reply": "2024-05-15T22:32:26.384163Z"
    },
    "papermill": {
     "duration": 40209.246562,
     "end_time": "2024-05-15T22:32:26.400403",
     "exception": false,
     "start_time": "2024-05-15T11:22:17.153841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "[0]\n",
      "[0 1]\n",
      "[0 1 2]\n",
      "[0 1 2 3]\n",
      "Training labels: (array([0, 1, 2, 3]), array([52552, 52547, 52403, 52497]))\n",
      "Validation labels: (array([0, 1, 2, 3]), array([7434, 7425, 7614, 7527]))\n",
      "Test labels: (array([0, 1, 2, 3]), array([15014, 15028, 14983, 14976]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b1-c27df63c.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b1-c27df63c.pth\n",
      "100%|██████████| 30.1M/30.1M [00:00<00:00, 105MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n",
      "Epoch 1/100, Training Loss: 1.0329\n",
      "Epoch 1/100, Validation Loss: 0.9110\n",
      "Saving the best model...\n",
      "Epoch 2/100, Training Loss: 0.9281\n",
      "Epoch 2/100, Validation Loss: 0.8824\n",
      "Saving the best model...\n",
      "Epoch 3/100, Training Loss: 0.8927\n",
      "Epoch 3/100, Validation Loss: 0.8685\n",
      "Saving the best model...\n",
      "Epoch 4/100, Training Loss: 0.8655\n",
      "Epoch 4/100, Validation Loss: 0.8488\n",
      "Saving the best model...\n",
      "Epoch 5/100, Training Loss: 0.8420\n",
      "Epoch 5/100, Validation Loss: 0.8366\n",
      "Saving the best model...\n",
      "Epoch 6/100, Training Loss: 0.8224\n",
      "Epoch 6/100, Validation Loss: 0.8086\n",
      "Saving the best model...\n",
      "Epoch 7/100, Training Loss: 0.8052\n",
      "Epoch 7/100, Validation Loss: 0.7960\n",
      "Saving the best model...\n",
      "Epoch 8/100, Training Loss: 0.7876\n",
      "Epoch 8/100, Validation Loss: 0.8242\n",
      "Epoch 9/100, Training Loss: 0.7711\n",
      "Epoch 9/100, Validation Loss: 0.7969\n",
      "Epoch 10/100, Training Loss: 0.7569\n",
      "Epoch 10/100, Validation Loss: 0.7845\n",
      "Saving the best model...\n",
      "Epoch 11/100, Training Loss: 0.7426\n",
      "Epoch 11/100, Validation Loss: 0.7620\n",
      "Saving the best model...\n",
      "Epoch 12/100, Training Loss: 0.7299\n",
      "Epoch 12/100, Validation Loss: 0.7548\n",
      "Saving the best model...\n",
      "Epoch 13/100, Training Loss: 0.7167\n",
      "Epoch 13/100, Validation Loss: 0.7431\n",
      "Saving the best model...\n",
      "Epoch 14/100, Training Loss: 0.7036\n",
      "Epoch 14/100, Validation Loss: 0.7337\n",
      "Saving the best model...\n",
      "Training stopped due to time limit.\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup\n",
    "import os\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Inicjalizacja modelu EfficientNet-B1\n",
    "def initialize_model(device, num_classes=4):\n",
    "    \"\"\"\n",
    "    Inicjalizuje model EfficientNet-B1 z odpowiednią liczbą klas i przenosi go na wybrane urządzenie.\n",
    "    Args:\n",
    "        device (torch.device): Urządzenie do trenowania (CPU lub GPU).\n",
    "        num_classes (int, opcjonalnie): Liczba klas w problemie klasyfikacji. Domyślnie 4.\n",
    "    Returns:\n",
    "        model (torch.nn.Module): Zainicjalizowany model EfficientNet-B1.\n",
    "    \"\"\"\n",
    "    weights = EfficientNet_B1_Weights.DEFAULT\n",
    "    model = efficientnet_b1(weights=weights)\n",
    "    num_features = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Linear(num_features, num_classes)\n",
    "    model = model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    return model\n",
    "\n",
    "class Alaska2Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Klasa dataset do ładowania danych Alaska2.\n",
    "\n",
    "    Args:\n",
    "        filenames (list): Lista ścieżek do plików obrazów.\n",
    "        labels (list): Lista etykiet odpowiadających plikom obrazów.\n",
    "        transform (callable, opcjonalnie): Funkcja transformacji stosowana do obrazów.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filenames, labels, transform=None):\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.filenames[idx]\n",
    "        image = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "# Loaders with train, val, and test split\n",
    "def prepare_loaders(base_dir, transform, test_size=0.2, val_size=0.1, random_state=42, batch_size=32, num_workers=4):\n",
    "    \"\"\"\n",
    "    Przygotowuje DataLoadery dla zbiorów trenowania, walidacji i testowania.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Katalog bazowy z danymi.\n",
    "        transform (callable): Funkcja transformacji stosowana do obrazów.\n",
    "        test_size (float, opcjonalnie): Procentowy rozmiar zbioru testowego. Domyślnie 0.2.\n",
    "        val_size (float, opcjonalnie): Procentowy rozmiar zbioru walidacyjnego. Domyślnie 0.1.\n",
    "        random_state (int, opcjonalnie): Stan losowy do podziału danych. Domyślnie 42.\n",
    "        batch_size (int, opcjonalnie): Rozmiar batcha. Domyślnie 32.\n",
    "        num_workers (int, opcjonalnie): Liczba wątków pracowników do ładowania danych. Domyślnie 4.\n",
    "\n",
    "    Returns:\n",
    "        train_loader (DataLoader): DataLoader dla zbioru trenowania.\n",
    "        val_loader (DataLoader): DataLoader dla zbioru walidacyjnego.\n",
    "        test_loader (DataLoader): DataLoader dla zbioru testowego.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    all_paths, all_labels = get_image_paths_and_labels(base_dir)\n",
    "    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
    "        all_paths, all_labels, test_size=(test_size + val_size), random_state=random_state\n",
    "    )\n",
    "    val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
    "        temp_paths, temp_labels, test_size=(test_size / (test_size + val_size)), random_state=random_state\n",
    "    )\n",
    "    train_dataset = Alaska2Dataset(train_paths, train_labels, transform)\n",
    "    val_dataset = Alaska2Dataset(val_paths, val_labels, transform)\n",
    "    test_dataset = Alaska2Dataset(test_paths, test_labels, transform)\n",
    "   \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    print(f\"Training labels: {np.unique(train_labels, return_counts=True)}\")\n",
    "    print(f\"Validation labels: {np.unique(val_labels, return_counts=True)}\")\n",
    "    print(f\"Test labels: {np.unique(test_labels, return_counts=True)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Paths and Labels\n",
    "def get_image_paths_and_labels(base_dir):\n",
    "    \"\"\"\n",
    "    Pobiera ścieżki do obrazów i odpowiadające im etykiety z katalogu bazowego.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Katalog bazowy z danymi.\n",
    "\n",
    "    Returns:\n",
    "        paths (list): Lista ścieżek do obrazów.\n",
    "        labels (list): Lista etykiet odpowiadających obrazom.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    dirs = ['Cover','JMiPOD', \"UERD\", \"JUNIWARD\"]\n",
    "    paths = []\n",
    "    labels = []\n",
    "    for idx, folder in enumerate(dirs):\n",
    "        full_path = os.path.join(base_dir, folder)\n",
    "        folder_paths = [os.path.join(full_path, file_name) for file_name in os.listdir(full_path) if file_name.endswith('.jpg')]\n",
    "        paths.extend(folder_paths)\n",
    "        \n",
    "        labels.extend([idx] * len(folder_paths))\n",
    "        \n",
    "        print(np.unique(labels))\n",
    "    return paths, labels\n",
    "\n",
    "\n",
    "\n",
    "# Training the model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10, patience=10, save_path='steganalysis_model_high_resolution.pt'):\n",
    "    \"\"\"\n",
    "    Trenuje model z wykorzystaniem podanych loaderów danych, kryterium, optymalizatora i urządzenia.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model do trenowania.\n",
    "        train_loader (DataLoader): DataLoader dla zbioru trenowania.\n",
    "        val_loader (DataLoader): DataLoader dla zbioru walidacyjnego.\n",
    "        criterion (torch.nn.Module): Funkcja strat.\n",
    "        optimizer (torch.optim.Optimizer): Optymalizator.\n",
    "        device (torch.device): Urządzenie do trenowania (CPU lub GPU).\n",
    "        num_epochs (int, opcjonalnie): Liczba epok trenowania. Domyślnie 10.\n",
    "        patience (int, opcjonalnie): Liczba epok do wczesnego zatrzymania bez poprawy. Domyślnie 10.\n",
    "        save_path (str, opcjonalnie): Ścieżka do zapisu najlepszego modelu. Domyślnie 'steganalysis_model_high_resolution.pt'.\n",
    "    \"\"\"\n",
    "   \n",
    "    scaler = GradScaler()\n",
    "    best_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    start_time = time.time()\n",
    "    max_duration = 11 * 3600  # 11 hours in seconds\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if time.time() - start_time > max_duration:\n",
    "            print(\"Training stopped due to time limit.\")\n",
    "            break\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "        val_loss = validate_model(model, val_loader, criterion, device)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\"Saving the best model...\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping! No improvement in validation loss for {patience} epochs.')\n",
    "                break\n",
    "    \n",
    "    print(\"Training completed.\")\n",
    "\n",
    "# Validate the model\n",
    "def validate_model(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Waliduje model na podanym loaderze danych.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model do walidacji.\n",
    "        loader (DataLoader): DataLoader dla zbioru walidacyjnego.\n",
    "        criterion (torch.nn.Module): Funkcja strat.\n",
    "        device (torch.device): Urządzenie do walidacji (CPU lub GPU).\n",
    "\n",
    "    Returns:\n",
    "        loss (float): Średnia strata na zbiorze walidacyjnym.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    loss = running_loss / total_samples\n",
    "    return loss\n",
    "\n",
    "# Confusion matrix plotting\n",
    "def save_confusion_matrix_plot(true_labels, pred_labels, class_names, file_path):\n",
    "    \"\"\"\n",
    "    Zapisuje wykres macierzy pomyłek do pliku.\n",
    "    Args:\n",
    "        true_labels (list): Lista prawdziwych etykiet.\n",
    "        pred_labels (list): Lista przewidzianych etykiet.\n",
    "        class_names (list): Lista nazw klas.\n",
    "        file_path (str): Ścieżka do zapisu wykresu.\n",
    "    \"\"\"\n",
    "\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    cm = confusion_matrix(true_labels, pred_labels, labels=[0, 1, 2, 3])\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(file_path)\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # Ustawienie urządzenia do trenowania (GPU jeśli dostępne, w przeciwnym razie CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Transformacje stosowane do obrazów\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Katalog bazowy zawierający dane\n",
    "    base_dir = '/kaggle/input/alaska2-image-steganalysis'\n",
    "    batch_size = 60  \n",
    "    num_workers = 4\n",
    "\n",
    "    # Przygotowanie loaderów danych\n",
    "    train_loader, val_loader, test_loader = prepare_loaders(base_dir, transform, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # Inicjalizacja modelu\n",
    "    num_classes = 4\n",
    "    model = initialize_model(device, num_classes=num_classes)\n",
    "\n",
    "    # Ustawienia do obliczania strat i optymalizatora\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Trenowanie modelu z dostosowaną szybkością uczenia i cierpliwością\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=100, patience=15)\n",
    "   \n",
    "    # Ewaluacja na zbiorze testowym\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    model.eval()  # Ustawienie modelu w trybie ewaluacji\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)  # Uzyskanie przewidywanej klasy\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Definiowanie nazw klas według danych\n",
    "    class_names = ['Cover', 'JMiPOD', 'UERD', 'JUNIWARD']\n",
    "\n",
    "    # Zapisanie wykresu macierzy pomyłek\n",
    "    save_confusion_matrix_plot(true_labels, pred_labels, class_names, 'confusion_matrix_test.png')\n",
    "   \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 1117522,
     "sourceId": 19991,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 40217.339373,
   "end_time": "2024-05-15T22:32:30.510962",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-15T11:22:13.171589",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
