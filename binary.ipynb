{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e23d6a3d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-15T11:21:28.268196Z",
     "iopub.status.busy": "2024-05-15T11:21:28.267839Z",
     "iopub.status.idle": "2024-05-15T18:36:38.868008Z",
     "shell.execute_reply": "2024-05-15T18:36:38.866922Z"
    },
    "papermill": {
     "duration": 26110.607667,
     "end_time": "2024-05-15T18:36:38.870740",
     "exception": false,
     "start_time": "2024-05-15T11:21:28.263073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b1-c27df63c.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b1-c27df63c.pth\n",
      "100%|██████████| 30.1M/30.1M [00:00<00:00, 70.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n",
      "Epoch 1/100, Training Loss: 0.6441\n",
      "Epoch 1/100, Validation Loss: 0.6157\n",
      "Saving the best model...\n",
      "Epoch 2/100, Training Loss: 0.6122\n",
      "Epoch 2/100, Validation Loss: 0.5966\n",
      "Saving the best model...\n",
      "Epoch 3/100, Training Loss: 0.5952\n",
      "Epoch 3/100, Validation Loss: 0.5854\n",
      "Saving the best model...\n",
      "Epoch 4/100, Training Loss: 0.5853\n",
      "Epoch 4/100, Validation Loss: 0.5794\n",
      "Saving the best model...\n",
      "Epoch 5/100, Training Loss: 0.5779\n",
      "Epoch 5/100, Validation Loss: 0.5940\n",
      "Epoch 6/100, Training Loss: 0.5693\n",
      "Epoch 6/100, Validation Loss: 0.5795\n",
      "Epoch 7/100, Training Loss: 0.5629\n",
      "Epoch 7/100, Validation Loss: 0.5752\n",
      "Saving the best model...\n",
      "Epoch 8/100, Training Loss: 0.5555\n",
      "Epoch 8/100, Validation Loss: 0.5906\n",
      "Epoch 9/100, Training Loss: 0.5493\n",
      "Epoch 9/100, Validation Loss: 0.5778\n",
      "Epoch 10/100, Training Loss: 0.5420\n",
      "Epoch 10/100, Validation Loss: 0.5819\n",
      "Epoch 11/100, Training Loss: 0.5348\n",
      "Epoch 11/100, Validation Loss: 0.6073\n",
      "Epoch 12/100, Training Loss: 0.5271\n",
      "Epoch 12/100, Validation Loss: 0.5883\n",
      "Epoch 13/100, Training Loss: 0.5209\n",
      "Epoch 13/100, Validation Loss: 0.6121\n",
      "Epoch 14/100, Training Loss: 0.5111\n",
      "Epoch 14/100, Validation Loss: 0.6193\n",
      "Epoch 15/100, Training Loss: 0.5032\n",
      "Epoch 15/100, Validation Loss: 0.6525\n",
      "Epoch 16/100, Training Loss: 0.4942\n",
      "Epoch 16/100, Validation Loss: 0.6496\n",
      "Epoch 17/100, Training Loss: 0.4849\n",
      "Epoch 17/100, Validation Loss: 0.6744\n",
      "Epoch 18/100, Training Loss: 0.4753\n",
      "Epoch 18/100, Validation Loss: 0.6540\n",
      "Epoch 19/100, Training Loss: 0.4669\n",
      "Epoch 19/100, Validation Loss: 0.7172\n",
      "Epoch 20/100, Training Loss: 0.4574\n",
      "Epoch 20/100, Validation Loss: 0.7784\n",
      "Epoch 21/100, Training Loss: 0.4477\n",
      "Epoch 21/100, Validation Loss: 0.7486\n",
      "Epoch 22/100, Training Loss: 0.4390\n",
      "Epoch 22/100, Validation Loss: 0.7574\n",
      "Early stopping! No improvement in validation loss for 15 epochs.\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup\n",
    "import os\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "import random\n",
    "\n",
    "def initialize_model(device, num_classes=2):\n",
    "    weights = EfficientNet_B1_Weights.DEFAULT\n",
    "    model = efficientnet_b1(weights=weights)\n",
    "    num_features = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Linear(num_features, num_classes)  \n",
    "    model = model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    return model\n",
    "\n",
    "class Alaska2Dataset(Dataset):\n",
    "    def __init__(self, filenames, labels, transform=None):\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.filenames[idx]\n",
    "        image = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "def prepare_loaders(base_dir, transform, val_size=0.2, test_size=0.2, random_state=42, batch_size=32, num_workers=4):\n",
    "    all_paths, all_labels = get_image_paths_and_labels(base_dir)\n",
    "\n",
    "    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
    "        all_paths, all_labels, test_size=val_size + test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    valid_size = val_size / (val_size + test_size)\n",
    "    valid_paths, test_paths, valid_labels, test_labels = train_test_split(\n",
    "        temp_paths, temp_labels, test_size=1-valid_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    train_dataset = Alaska2Dataset(train_paths, train_labels, transform)\n",
    "    valid_dataset = Alaska2Dataset(valid_paths, valid_labels, transform)\n",
    "    test_dataset = Alaska2Dataset(test_paths, test_labels, transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "def get_image_paths_and_labels(base_dir):\n",
    "    manipulated_dirs = ['JMiPOD', 'UERD', 'JUNIWARD']\n",
    "    cover_dir = 'Cover'\n",
    "    manipulated_paths = []\n",
    "    \n",
    "    # Gather paths for manipulated and cover images\n",
    "    for folder in manipulated_dirs:\n",
    "        full_path = os.path.join(base_dir, folder)\n",
    "        folder_paths = [os.path.join(full_path, file_name) for file_name in os.listdir(full_path) if file_name.endswith('.jpg')]\n",
    "        manipulated_paths.extend(folder_paths)\n",
    "        \n",
    "    full_path = os.path.join(base_dir, cover_dir)\n",
    "    cover_paths = [os.path.join(full_path, file_name) for file_name in os.listdir(full_path) if file_name.endswith('.jpg')]\n",
    "    \n",
    "    min_count = min(len(manipulated_paths), len(cover_paths))\n",
    "    \n",
    "    manipulated_paths = random.sample(manipulated_paths, min_count)\n",
    "    cover_paths = random.sample(cover_paths, min_count)\n",
    "    \n",
    "    paths = manipulated_paths + cover_paths\n",
    "    labels = [1] * min_count + [0] * min_count  # 1 for manipulated, 0 for not manipulated\n",
    "    \n",
    "    return paths, labels\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, device, num_epochs=10, patience=10, save_path='steganalysis_model_binary.pt'):\n",
    "    scaler = GradScaler()\n",
    "    best_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    start_time = time.time()\n",
    "    max_duration = 11 * 3600  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if time.time() - start_time > max_duration:\n",
    "            print(\"Training stopped due to time limit.\")\n",
    "            break\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        valid_loss = validate_model(model, valid_loader, criterion, device)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(\"Saving the best model...\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping! No improvement in validation loss for {patience} epochs.')\n",
    "                break\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_confusion_matrix_plot(true_labels, pred_labels, class_names, file_path):\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(file_path)\n",
    "    plt.close()\n",
    "\n",
    "def validate_model(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    valid_loss = running_loss / total_samples\n",
    "    return valid_loss\n",
    "\n",
    "def predict_probability(model, dataloader, device):\n",
    "    model.eval()\n",
    "    probabilities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = nn.Softmax(dim=1)(outputs)  \n",
    "            probabilities.extend(probs[:, 1].cpu().numpy())  \n",
    "\n",
    "    return probabilities\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    base_dir = '/kaggle/input/alaska2-image-steganalysis'\n",
    "    batch_size = 80\n",
    "    num_workers = 4\n",
    "\n",
    "    train_loader, valid_loader, test_loader = prepare_loaders(base_dir, transform, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    model = initialize_model(device, num_classes=2)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_model(model, train_loader, valid_loader, criterion, optimizer, device, num_epochs=100, patience=15)\n",
    "\n",
    "    \n",
    "    true_labels, pred_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    class_names = ['NotManipulated','Manipulated']\n",
    "\n",
    "    save_confusion_matrix_plot(true_labels, pred_labels, class_names, 'confusion_matrix_test.png')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6c8c80",
   "metadata": {
    "papermill": {
     "duration": 0.006105,
     "end_time": "2024-05-15T18:36:38.883682",
     "exception": false,
     "start_time": "2024-05-15T18:36:38.877577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778d980c",
   "metadata": {
    "papermill": {
     "duration": 0.005789,
     "end_time": "2024-05-15T18:36:38.895496",
     "exception": false,
     "start_time": "2024-05-15T18:36:38.889707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 1117522,
     "sourceId": 19991,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26118.030917,
   "end_time": "2024-05-15T18:36:42.171451",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-15T11:21:24.140534",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
